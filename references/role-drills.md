# Role-Specific Drills

These drills simulate the scrutiny candidates face from specialists in their field. Run weekly, not just before interviews.

---

## Product Manager: Six-Lens Stress Test

**Setup**: Candidate describes a product decision or initiative they led.

**Challenge from 6 perspectives**, progressively harder based on performance:

### 1. Engineering Lens
- "This sounds like 6 months of work. How'd you scope it?"
- "What technical debt did this create?"
- "How'd you handle the infrastructure limitations?"
- "Walk me through the build vs. buy decision."
- "What did engineering push back on, and how did you resolve it?"

### 2. Design Lens
- "What user research backed this decision?"
- "How did you balance user needs vs. business goals?"
- "What did you sacrifice for simplicity?"
- "How many iterations did the design go through? What changed?"
- "What did users hate that you shipped anyway? Why?"

### 3. Data Lens
- "How did you measure success?"
- "What was your null hypothesis?"
- "How did you handle selection bias in your metrics?"
- "What metrics didn't move that you expected to?"
- "If I looked at your data, what would concern me?"

### 4. Business Lens
- "Walk me through the unit economics."
- "How'd this impact your growth loops?"
- "What was the opportunity cost of this vs. other projects?"
- "How did this affect revenue/retention/engagement?"
- "What would have happened if you'd done nothing?"

### 5. Competitor Lens
- "Competitor X tried this and failed. Why'd you succeed?"
- "How'd this affect your positioning?"
- "What stops them from copying this tomorrow?"
- "Who else considered this approach and abandoned it?"

### 6. Skeptic Lens
- "This seems like a solution looking for a problem."
- "Your success metrics feel cherry-picked."
- "How do you know this wasn't just regression to the mean?"
- "Sounds like you got lucky. What was actually skill?"
- "What would you do differently with hindsight?"

**Scoring per response:**
- Acknowledging the tension (vs. dismissing it): 1-5
- Specific evidence (vs. hand-waving): 1-5
- Admitting uncertainty (vs. false confidence): 1-5

---

## Software Engineer: Technical Depth Test

**Setup**: Candidate describes a technical project or system they built.

**Challenge across dimensions:**

### Architecture
- "Draw the system architecture. Where are the bottlenecks?"
- "What happens at 10x scale? 100x?"
- "What would you redesign if starting over?"
- "Where did you take shortcuts? What's the tech debt?"

### Trade-offs
- "Why this stack over alternatives?"
- "What did you optimize for? What did you sacrifice?"
- "How did you balance speed vs. quality?"
- "What's the maintenance burden of this approach?"

### Debugging
- "Walk me through the hardest bug you encountered."
- "How did you identify the root cause?"
- "What monitoring would have caught this earlier?"
- "How do you know it's actually fixed?"

### Collaboration
- "How did you handle disagreements on technical approach?"
- "How did you communicate technical constraints to non-engineers?"
- "What did you learn from code review feedback?"
- "How did you onboard others to this codebase?"

### Edge Cases
- "What happens when [component] fails?"
- "How do you handle [unusual input]?"
- "What's your rollback strategy?"
- "What security considerations did you address?"

**Scoring per response:**
- Technical accuracy: 1-5
- Depth of understanding (not just surface): 1-5
- Awareness of trade-offs and alternatives: 1-5

---

## Designer: Critique and Rationale Test

**Setup**: Candidate presents a design project (ideally with visuals, but verbal walkthrough works).

**Challenge across dimensions:**

### Research Foundation
- "What research informed this direction?"
- "How many users did you talk to? What surprised you?"
- "What did the data say that contradicted your intuition?"
- "How did you validate this solved the actual problem?"

### Design Rationale
- "Why this layout over alternatives?"
- "Walk me through your information hierarchy decisions."
- "What did you try that didn't work?"
- "How did you balance aesthetics vs. usability?"

### Constraints
- "What technical constraints shaped the design?"
- "How did you work within brand guidelines?"
- "What did you fight for that got cut?"
- "How did you handle stakeholder feedback you disagreed with?"

### Accessibility & Edge Cases
- "How does this work for users with [disability]?"
- "What happens on mobile? Slow connections?"
- "How does this scale with 10x content?"
- "What's the empty state? Error state?"

### Impact
- "How did you measure design success?"
- "What behavioral change did you observe?"
- "What would you improve in V2?"
- "How did this affect key metrics?"

**Scoring per response:**
- User-centeredness (vs. assumption-driven): 1-5
- Rationale clarity (can explain "why"): 1-5
- Openness to critique (vs. defensive): 1-5

---

## Data Scientist: Methodology Rigor Test

**Setup**: Candidate describes an analysis, model, or data project.

**Challenge across dimensions:**

### Problem Framing
- "How did you define the problem? Who defined success?"
- "What was the business question behind the technical question?"
- "What would 'wrong' look like? How would you know?"

### Data Quality
- "Where did the data come from? What's missing?"
- "How did you handle missing values? Outliers?"
- "What biases exist in this dataset?"
- "How representative is your sample?"

### Methodology
- "Why this approach over alternatives?"
- "What assumptions does this method require?"
- "How did you validate those assumptions?"
- "Walk me through your feature engineering decisions."

### Evaluation
- "What metrics did you optimize for? Why those?"
- "How did you prevent overfitting?"
- "What's your confidence interval? Statistical significance?"
- "How did you validate in production vs. test?"

### Communication
- "How did you explain this to non-technical stakeholders?"
- "What pushback did you get? How did you address it?"
- "What did you simplify for the audience? What did you lose?"

**Scoring per response:**
- Statistical rigor: 1-5
- Awareness of limitations: 1-5
- Business translation (not just technical): 1-5

---

## UX Researcher: Evidence and Influence Test

**Setup**: Candidate describes a research project and its impact.

**Challenge across dimensions:**

### Study Design
- "Why this method over alternatives?"
- "What's your sample size? How did you recruit?"
- "What biases might affect your findings?"
- "How did you ensure you weren't leading participants?"

### Analysis
- "How did you synthesize across participants?"
- "What patterns emerged? What outliers did you see?"
- "How did you distinguish signal from noise?"
- "What surprised you vs. confirmed expectations?"

### Insight Quality
- "What's the 'so what' of this finding?"
- "How actionable is this insight?"
- "What does this NOT tell us?"
- "How confident are you? What would change your mind?"

### Influence
- "How did you get stakeholders to act on this?"
- "Who disagreed? How did you handle it?"
- "What research did NOT lead to change? Why?"
- "How did you prioritize which findings to push?"

### Ethics
- "How did you handle sensitive participant data?"
- "What consent did participants give?"
- "Were there findings you chose not to share? Why?"

**Scoring per response:**
- Methodological soundness: 1-5
- Insight actionability: 1-5
- Stakeholder influence: 1-5

---

## Operations / Business Ops: Systems Thinking Test

**Setup**: Candidate describes a process, system, or operational improvement they led.

**Challenge across dimensions:**

### Problem Diagnosis
- "How did you identify this was the right problem to solve?"
- "What was the root cause vs. symptoms?"
- "What data told you this was worth fixing?"
- "What was the cost of doing nothing?"

### Solution Design
- "What alternatives did you consider?"
- "Why this approach over others?"
- "What dependencies did you uncover?"
- "How did you handle edge cases?"

### Implementation
- "How did you roll this out? Phased or all at once?"
- "What resistance did you encounter?"
- "How did you get buy-in from stakeholders?"
- "What broke during implementation?"

### Measurement
- "How did you measure success?"
- "What leading vs. lagging indicators did you track?"
- "How did you isolate impact from other changes?"
- "What didn't improve that you expected to?"

### Sustainability
- "How did you ensure this stuck after you moved on?"
- "What documentation/training did you create?"
- "Who owns this now?"
- "What maintenance burden did you create?"

**Scoring per response:**
- Systems thinking (sees connections): 1-5
- Change management awareness: 1-5
- Measurement rigor: 1-5

---

## Marketing: Strategy and Attribution Test

**Setup**: Candidate describes a campaign, launch, or marketing initiative.

**Challenge across dimensions:**

### Strategy
- "Why this channel/approach over alternatives?"
- "Who was the target audience? How did you define them?"
- "What was the competitive context?"
- "How did this fit into the broader marketing strategy?"

### Execution
- "Walk me through the timeline and key decisions."
- "What did you have to cut or change mid-flight?"
- "How did you work with creative/product/sales?"
- "What was your budget? How did you allocate it?"

### Attribution
- "How did you measure impact?"
- "What was your attribution model? What are its flaws?"
- "How did you separate this campaign's impact from other factors?"
- "What metrics didn't move that you expected to?"

### Learning
- "What would you do differently?"
- "What did you learn about the audience?"
- "How did this inform future campaigns?"
- "What surprised you about performance?"

**Scoring per response:**
- Strategic clarity: 1-5
- Measurement sophistication: 1-5
- Learning orientation: 1-5

---

## Technical Communication Drills

These drills target the communication skills needed for system design, case study, and mixed-format interviews. They are **not domain-specific technical practice** — they coach how you communicate technical thinking, not what you know. Use alongside the Format Discovery Protocol and Technical Format Coaching Boundaries defined in `references/workflows.md`.

See `references/workflows.md` for the `practice technical` menu entry and progression ladder placement.

### Thinking Out Loud Drill

**Setup**: Give the candidate an open-ended decision scenario — not a full system design problem, but a decision point that requires structured reasoning. Examples:

- "Your team needs to decide between building a feature in-house or using a third-party service. Walk me through how you'd think about this decision."
- "You've been asked to reduce page load time by 50%. Where do you start, and how do you decide what to prioritize?"
- "Your team is debating whether to migrate to a new framework. Walk me through how you'd evaluate the decision."

Adapt the scenario to the candidate's role and target company if available (from `coaching_state.md`).

**What to evaluate** (communication, not correctness):

- Did they state their assumptions before diving in?
- Did they structure their approach before detailing it ("I'd think about this in three dimensions...")?
- Did they narrate their reasoning ("I'm leaning toward X because...")?
- Did they address tradeoffs explicitly and unprompted?
- Did they acknowledge what they don't know?

**Challenge dimensions:**

- "You've been talking for 2 minutes and I still don't know your recommendation. What's your answer?" (tests ability to land a conclusion under pressure)
- "You said you'd optimize for X. What are you giving up?" (tests tradeoff articulation)
- "I disagree with your assumption about [thing they assumed]. What changes?" (tests adaptability when a premise is challenged)
- "Can you summarize your approach in 30 seconds?" (tests compression — same skill as the constraint ladder, applied to technical thinking)

**Scoring per response:**

- Process visibility (could the interviewer follow your thinking?): 1-5
- Structure (did you organize before detailing?): 1-5
- Tradeoff awareness (did you name what you're sacrificing?): 1-5

### Clarification-Seeking Drill

**Setup**: Present an intentionally ambiguous prompt with no additional context. The goal is to evaluate whether the candidate scopes the problem before solving — a critical skill in system design interviews that most candidates skip.

Examples:

- "Design a notification system."
- "Build a recommendation engine."
- "Create a dashboard for the operations team."
- "Improve the checkout flow."

Give NO additional context. Wait silently. Evaluate what they do next.

**What to evaluate:**

- Did they ask clarifying questions before starting to solve? (most candidates don't — they jump straight in)
- Quality of questions: scoping questions ("Who are the users? What scale?") vs. trivial questions ("What programming language?")
- Did they state assumptions explicitly when they couldn't get answers? ("I'll assume we're talking about web and mobile, at the scale of about 10M users — let me know if that's wrong.")
- Comfort with ambiguity: did they proceed productively, or freeze waiting for certainty?

**Challenge dimensions:**

- If they jump straight to solving: "Hold on — what problem are you solving, exactly? Who's it for? At what scale?" (forces them to recognize they didn't scope — don't be gentle about this, it's a make-or-break habit)
- After they ask questions: "I can't tell you that. What would you assume and why?" (tests ability to proceed under ambiguity with transparent assumptions)
- "You asked about users but not about constraints. Why?" (tests awareness of their own question gaps)
- "You asked five questions in a row without starting. At some point you need to move forward with incomplete information. When is that point?" (tests the other extreme — over-clarifying as a stalling mechanism)

**Scoring per response:**

- Question quality (scoping vs. trivial): 1-5
- Assumption transparency (stated vs. hidden): 1-5
- Comfort with ambiguity (productive vs. paralyzed or overcorrecting): 1-5

### Mode-Switching Drill

**Setup**: This drill targets candidates preparing for technical+behavioral mix interviews. Start with a behavioral question about a technical decision, then pivot to technical discussion mid-answer, then pivot back to behavioral.

Example sequence:

1. Start behavioral: "Tell me about a time you had to make a difficult technical tradeoff."
2. Mid-answer, pivot to technical: "Interesting — walk me through the technical approach you chose. What were the alternatives and why did you pick this one?"
3. Pivot back to behavioral: "How did the team respond to that decision? Was there disagreement?"
4. Pivot to forward-looking technical: "If you faced a similar problem today with different constraints, how would your approach change?"

**What to evaluate:**

- Speed of register shift: does the candidate switch fluidly, or fumble transitions?
- Depth maintenance: do they maintain technical depth when in behavioral mode and behavioral warmth when in technical mode?
- Integration: do the modes reinforce each other, or feel like two different candidates?
- Energy consistency: does one mode visibly drain them more than the other?

**Challenge dimensions:**

- Rapid pivots: switch mode 3 times in 5 minutes without signposting
- "You sound like a different person when you talk about the technical side. How do you bring that same energy to the people side?" (meta-feedback that reveals mode-switching patterns)
- "That story was great, but I still don't understand the technical decision. Walk me through it like I'm an engineer on your team." (tests whether behavioral warmth comes at the cost of technical rigor)

**Scoring per response:**

- Mode-switching fluidity (seamless vs. fumbled): 1-5
- Register appropriateness (depth in both modes): 1-5
- Integration (modes reinforce each other vs. disconnected): 1-5

### Tradeoff Articulation Drill

**Setup**: Present a decision with no clear right answer. The point is not to find the "correct" answer — it's to practice making a decision defensible by naming what you're optimizing for, what you're giving up, and why.

Examples:

- "You can ship in 2 weeks with known technical debt, or in 6 weeks with a clean architecture. Walk me through your decision process."
- "You have budget for either a senior hire or two junior hires. What do you choose and why?"
- "Your team can build a feature that 80% of users want a little, or a feature that 20% of users want desperately. Which do you build?"
- "You can optimize for latency or for cost. The business says both matter equally. How do you decide?"

Adapt examples to the candidate's role and domain.

**What to evaluate:**

- Did they name the tradeoff explicitly, or treat it as if there's an obvious right answer?
- Did they identify what they were optimizing for?
- Did they acknowledge what they were giving up?
- Did they tie the decision to context (team size, timeline, business stage) rather than abstract principles?
- Did they show intellectual honesty about downsides, or sell their choice as if it has no cost?

**Challenge dimensions:**

- "Your manager wants the 2-week version. Your tech lead wants the 6-week version. What do you do?" (adds stakeholder complexity)
- "A year later, the technical debt is causing production incidents. How do you think about your original decision?" (tests intellectual honesty and hindsight reasoning)
- "What information would change your answer?" (tests decision framework quality — do they know what would flip their conclusion?)
- "I would have made the opposite choice. Convince me." (tests ability to defend a position under direct challenge without getting defensive)

**Scoring per response:**

- Tradeoff naming (explicit vs. glossed over): 1-5
- Context sensitivity (decision tied to situation vs. abstract principles): 1-5
- Intellectual honesty (acknowledges downsides vs. sells the choice): 1-5

### Role Adaptations for Technical Communication Drills

The drills above are role-agnostic, but the scenarios should be adapted to each candidate's domain:

- **Software Engineer**: Use architecture, infrastructure, and system design scenarios. Thinking Out Loud → "design a [system]" prompts. Clarification-Seeking → "build a service that handles [ambiguous requirement]."
- **Product Manager**: Use prioritization, roadmap, and resource allocation scenarios. Tradeoff Articulation maps directly to roadmap decisions. Clarification-Seeking → "improve [product area]."
- **Data Scientist**: Use methodology, model selection, and experiment design scenarios. Thinking Out Loud → "how would you measure [ambiguous metric]?" Clarification-Seeking → "build a model to predict [X]."
- **Designer**: Use design system, UX architecture, and design critique scenarios. Thinking Out Loud → "redesign [flow]." Tradeoff Articulation → "user delight vs. development cost."
- **Operations / Business Ops**: Use process design, tool selection, and systems improvement scenarios. Tradeoff Articulation → "build vs. buy" and "automate vs. manual" decisions.
- **Marketing**: Use campaign strategy, channel allocation, and measurement scenarios. Tradeoff Articulation → "brand awareness vs. direct response" and "broad reach vs. targeted conversion."

Pull from the candidate's target companies and roles (from `coaching_state.md`) to make scenarios specific. Generic scenarios build the skill; role-specific scenarios build confidence.

---

## Interviewer Archetypes (for Panel Simulation)

When running `practice panel` or `mock` with panel format, deploy 2-3 of these distinct interviewer personas simultaneously. The candidate must learn to manage different interpersonal dynamics in one conversation.

### The Friendly Ally
- **Behavior**: Warm, encouraging, asks softball questions, builds rapport
- **Purpose**: Tests whether candidate stays rigorous when given easy questions, or becomes lazy/casual
- **Typical questions**: "That's fascinating — tell me more about that." / "What was the most rewarding part?"
- **What they're actually evaluating**: Depth of substance even when not pressured; whether warmth makes the candidate drop their guard
- **Danger for candidate**: Over-sharing, getting too casual, losing structure because the vibe is comfortable

### The Skeptic
- **Behavior**: Arms crossed, challenges every claim, asks "how do you know?" repeatedly
- **Purpose**: Tests defensiveness vs. intellectual curiosity under pressure
- **Typical questions**: "I'm not convinced — what evidence do you have?" / "That sounds like correlation, not causation." / "Your success metrics feel cherry-picked."
- **What they're actually evaluating**: Can you engage with skepticism without getting defensive? Do you acknowledge limitations?
- **Danger for candidate**: Getting defensive, over-explaining, losing confidence, or caving too quickly

### The Silent Observer
- **Behavior**: Takes notes, rarely speaks, minimal facial expressions, occasional nod
- **Purpose**: Tests composure without feedback; creates discomfort that reveals nerves
- **Typical questions**: Asks one or two precise, surgical questions late in the conversation
- **What they're actually evaluating**: How does the candidate perform without validation? Do they fill silence with rambling?
- **Danger for candidate**: Over-talking to fill the silence, getting anxious about lack of feedback, directing all attention to the more responsive panelists

### The Off-Script Wanderer
- **Behavior**: Asks unexpected questions unrelated to standard prep, goes on tangents, seems distracted
- **Purpose**: Tests adaptability and real-time thinking
- **Typical questions**: "Forget the project — what do you think about [industry trend]?" / "If you could redesign [company product] from scratch, what would you change?" / "What's something you believe that most people in your field disagree with?"
- **What they're actually evaluating**: Can you think on your feet? Do you have depth beyond prepared stories? Do you have genuine intellectual curiosity?
- **Danger for candidate**: Freezing on unexpected questions, trying to redirect back to prepared material instead of engaging

### The Time-Pressured Exec
- **Behavior**: Checks watch, speaks quickly, interrupts, wants bottom-line answers
- **Purpose**: Tests ability to compress and prioritize under time pressure
- **Typical questions**: "Give me the 30-second version." / "Skip the background — what was the outcome?" / "I have 5 minutes left — what's the one thing you want me to know?"
- **What they're actually evaluating**: Clarity under pressure. Can you get to the point? Do you know what matters?
- **Danger for candidate**: Rambling, starting with context instead of conclusions, failing to read the urgency signal

### The Culture Guardian
- **Behavior**: Friendly but probing on values, team dynamics, and how you treat people
- **Purpose**: Tests cultural fit and interpersonal judgment
- **Typical questions**: "Tell me about a time you disagreed with your manager." / "How did you handle someone on your team who was underperforming?" / "What kind of team culture do you create?"
- **What they're actually evaluating**: Empathy, self-awareness, conflict resolution style, whether your values actually align
- **Danger for candidate**: Giving "right" answers that sound performative, not demonstrating authentic values

### Panel Simulation Protocol

When running a panel drill:
1. Assign 2-3 archetypes (vary the combination each time).
2. Switch between personas naturally — don't announce "now the skeptic will ask."
3. Create moments where two interviewers' styles conflict (e.g., the Ally is encouraging while the Skeptic challenges the same point).
4. Debrief: How did the candidate manage competing dynamics? Did they play to one persona and neglect another? Did they read the room?

---

## High-Pressure Stress Drill (`practice stress`)

This drill simulates the worst-case version of an interview — multiple stressors layered simultaneously. It's the final stress test before the real thing. Only run after the candidate has passed Stages 1-5 in the progression ladder.

**Purpose**: Build resilience when everything goes wrong at once. Real interviews don't stress-test one dimension at a time — they combine time pressure, skepticism, unexpected questions, and format curveballs simultaneously.

### Setup

1. **Pull the candidate's weakest patterns** from `coaching_state.md` — Active Patterns, Score History, and Revisit Queue. The stress drill should target known vulnerabilities, not random pressure.
2. **Select the role-specific drill** that matches their target (PM Six-Lens, Engineer Technical Depth, etc.). The stress drill layers pressure *on top of* the role-specific content.
3. **Set the frame**: "This is the hardest drill I'll throw at you. It's designed to break your composure so we can see what happens when things go sideways. Don't try to be perfect — try to recover."

### Stress Layers (apply 3-4 simultaneously)

| Stressor | How to Apply | What It Tests |
|---|---|---|
| **Time compression** | "You have 60 seconds for this answer." Apply to questions that normally take 2-3 minutes. | Prioritization under pressure — can they find the headline fast? |
| **Hostile interviewer** | Combine the Skeptic and Time-Pressured Exec archetypes. Interrupt early, challenge every claim, show impatience. | Composure and defensiveness management |
| **Curveball sequencing** | Ask a question from a completely different domain mid-flow. Go from behavioral to "What's your biggest weakness?" to technical without transition. | Cognitive flexibility and recovery speed |
| **Information denial** | When they ask clarifying questions, say "I can't tell you that — just go with your best judgment." Repeatedly. | Comfort with ambiguity and assumption transparency |
| **Previous-answer callback** | Reference something they said 3 questions ago and challenge it: "Earlier you said X, but now you're saying Y — which is it?" | Consistency and composure when caught in a contradiction (real or perceived) |
| **Silence after answer** | After their response, wait 5+ seconds. Say nothing. Then: "Is that your final answer, or do you want to add anything?" | Resistance to panic-filling silence |

### Protocol

1. Run 4-5 questions with 3-4 stressors active per question. Vary which stressors are applied.
2. Do NOT debrief between questions — maintain pressure throughout, like a real interview.
3. After the full sequence, give the candidate 30 seconds of silence before debriefing. They need to decompress.
4. **Debrief the recovery, not the content.** The point isn't whether their answers were perfect — it's how they handled the pressure:
   - Where did composure hold? Where did it break?
   - What was their recovery time after a bad moment? (Instant? One question? Never recovered?)
   - Did stress make them verbose or clipped? (Both are failure modes)
   - Did they maintain their Structure scores under pressure, or did narrative architecture collapse first?

### Scoring Per Round

- Composure maintenance (stays grounded vs. spirals): 1-5
- Recovery speed (bounces back vs. carries bad energy forward): 1-5
- Content quality under pressure (substance holds vs. goes hollow): 1-5

### Role-Specific Stress Variants

- **PM**: Layer Business Lens + Skeptic Lens challenges simultaneously. Add a "the CEO just asked about this in the hallway" urgency framing.
- **Engineer**: Ask them to explain a technical decision to a non-technical exec, then immediately pivot to a deep technical probe from a senior engineer persona. Test register-switching under time pressure.
- **Designer**: Present harsh design critique mid-answer: "Our users wouldn't use this." Test whether they get defensive or curious.
- **Data Scientist**: Challenge statistical methodology with a pointed "But couldn't you have just..." question that proposes a simpler approach. Test whether they defend rigor without being dismissive.
- **Operations**: Introduce a new constraint mid-answer that invalidates their approach: "Actually, the timeline just got cut in half." Test real-time re-planning.
- **Marketing**: Challenge attribution claims aggressively: "How do you know it wasn't just seasonal?" Layer with "And the budget for this was how much?"

---

## Difficulty Scaling Guide

Within each role drill, questions range from entry-level to advanced. Use this guide to calibrate which questions to deploy at each stage of the candidate's progression.

### How to Scale

- **Warm-up (rounds 1-2)**: Start with open-ended questions that let the candidate choose their best material. Lower dimensions: "Walk me through...", "Tell me about..."
- **Moderate (rounds 3-4)**: Add specificity and constraints. "Walk me through the unit economics" is harder than "How'd this impact revenue?" Force them to show depth, not just breadth.
- **Advanced (rounds 5+)**: Deploy the hardest questions in each lens — the ones that expose gaps. Skeptic Lens questions, compound challenges ("Your metrics look cherry-picked AND your timeline seems aggressive"), and questions that require admitting uncertainty.

### Role-Specific Difficulty Markers

**PM Six-Lens**: Engineering and Data lenses are typically hardest for PMs — deploy those later. Business Lens questions escalate from "How'd this impact revenue?" (moderate) to "Walk me through the unit economics" (advanced) to "What was the opportunity cost vs. the next best project?" (expert).

**Engineer Technical Depth**: Architecture and Edge Cases escalate most steeply. "What happens at 10x scale?" (moderate) → "What happens at 100x with half the infrastructure budget?" (advanced) → "Walk me through the failure mode when [component] and [component] both fail simultaneously" (expert).

**Designer Critique**: Research Foundation questions are the warmup. Constraints and Accessibility escalate from "How did you work within brand guidelines?" (moderate) to "The engineering team says this is impossible to build. What do you do?" (advanced).

**Data Scientist Methodology**: Problem Framing is the warmup. Evaluation and Methodology escalate from "What metrics did you optimize for?" (moderate) to "How did you prevent overfitting given your sample size?" (advanced) to "Your approach assumes stationarity — what breaks if that doesn't hold?" (expert).

---

## General Drill Guidelines

1. **Start easier, escalate**: Begin with straightforward questions, increase difficulty based on how well they handle early ones.

2. **Push on "we"**: When candidate says "we," ask "What was YOUR specific contribution?"

3. **Probe the negative**: Every project has things that went wrong. If they only share positives, dig.

4. **Test adaptability**: Throw a curveball mid-answer: "Actually, let me ask about a different aspect..."

5. **Score in real-time**: Keep notes on where they're strong vs. struggling for debrief.

6. **Debrief with coaching presence**: After each drill round:
   - Ask: "How did that feel? What would you change if you could do it again?"
   - Reflect back what you observed working well FIRST
   - Share observations on gaps as questions when possible: "I noticed you didn't mention the data behind that decision — was that a deliberate choice, or did it slip?"
   - Close with: "What's one thing you want to try differently on the next round?"

7. **Notice patterns across drills**: If someone consistently struggles with a particular challenge type (e.g., always gets defensive on skeptical pushback), name the pattern: "I'm seeing this come up again. What do you think is happening when you get that kind of question?"

8. **Celebrate growth within a session**: When they improve from round 1 to round 3, name it specifically: "Did you notice the difference there? Your answer on that last one was half the length and twice as clear. What changed?"
