# Role-Specific Drills

These drills simulate the scrutiny candidates face from specialists in their field. Run weekly, not just before interviews.

---

## Product Manager: Six-Lens Stress Test

**Setup**: Candidate describes a product decision or initiative they led.

**Challenge from 6 perspectives**, progressively harder based on performance:

### 1. Engineering Lens
- "This sounds like 6 months of work. How'd you scope it?"
- "What technical debt did this create?"
- "How'd you handle the infrastructure limitations?"
- "Walk me through the build vs. buy decision."
- "What did engineering push back on, and how did you resolve it?"

### 2. Design Lens
- "What user research backed this decision?"
- "How did you balance user needs vs. business goals?"
- "What did you sacrifice for simplicity?"
- "How many iterations did the design go through? What changed?"
- "What did users hate that you shipped anyway? Why?"

### 3. Data Lens
- "How did you measure success?"
- "What was your null hypothesis?"
- "How did you handle selection bias in your metrics?"
- "What metrics didn't move that you expected to?"
- "If I looked at your data, what would concern me?"

### 4. Business Lens
- "Walk me through the unit economics."
- "How'd this impact your growth loops?"
- "What was the opportunity cost of this vs. other projects?"
- "How did this affect revenue/retention/engagement?"
- "What would have happened if you'd done nothing?"

### 5. Competitor Lens
- "Competitor X tried this and failed. Why'd you succeed?"
- "How'd this affect your positioning?"
- "What stops them from copying this tomorrow?"
- "Who else considered this approach and abandoned it?"

### 6. Skeptic Lens
- "This seems like a solution looking for a problem."
- "Your success metrics feel cherry-picked."
- "How do you know this wasn't just regression to the mean?"
- "Sounds like you got lucky. What was actually skill?"
- "What would you do differently with hindsight?"

**Scoring per response:**
- Acknowledging the tension (vs. dismissing it): 1-5
- Specific evidence (vs. hand-waving): 1-5
- Admitting uncertainty (vs. false confidence): 1-5

---

## Software Engineer: Technical Depth Test

**Setup**: Candidate describes a technical project or system they built.

**Challenge across dimensions:**

### Architecture
- "Draw the system architecture. Where are the bottlenecks?"
- "What happens at 10x scale? 100x?"
- "What would you redesign if starting over?"
- "Where did you take shortcuts? What's the tech debt?"

### Trade-offs
- "Why this stack over alternatives?"
- "What did you optimize for? What did you sacrifice?"
- "How did you balance speed vs. quality?"
- "What's the maintenance burden of this approach?"

### Debugging
- "Walk me through the hardest bug you encountered."
- "How did you identify the root cause?"
- "What monitoring would have caught this earlier?"
- "How do you know it's actually fixed?"

### Collaboration
- "How did you handle disagreements on technical approach?"
- "How did you communicate technical constraints to non-engineers?"
- "What did you learn from code review feedback?"
- "How did you onboard others to this codebase?"

### Edge Cases
- "What happens when [component] fails?"
- "How do you handle [unusual input]?"
- "What's your rollback strategy?"
- "What security considerations did you address?"

**Scoring per response:**
- Technical accuracy: 1-5
- Depth of understanding (not just surface): 1-5
- Awareness of trade-offs and alternatives: 1-5

---

## Designer: Critique and Rationale Test

**Setup**: Candidate presents a design project (ideally with visuals, but verbal walkthrough works).

**Challenge across dimensions:**

### Research Foundation
- "What research informed this direction?"
- "How many users did you talk to? What surprised you?"
- "What did the data say that contradicted your intuition?"
- "How did you validate this solved the actual problem?"

### Design Rationale
- "Why this layout over alternatives?"
- "Walk me through your information hierarchy decisions."
- "What did you try that didn't work?"
- "How did you balance aesthetics vs. usability?"

### Constraints
- "What technical constraints shaped the design?"
- "How did you work within brand guidelines?"
- "What did you fight for that got cut?"
- "How did you handle stakeholder feedback you disagreed with?"

### Accessibility & Edge Cases
- "How does this work for users with [disability]?"
- "What happens on mobile? Slow connections?"
- "How does this scale with 10x content?"
- "What's the empty state? Error state?"

### Impact
- "How did you measure design success?"
- "What behavioral change did you observe?"
- "What would you improve in V2?"
- "How did this affect key metrics?"

**Scoring per response:**
- User-centeredness (vs. assumption-driven): 1-5
- Rationale clarity (can explain "why"): 1-5
- Openness to critique (vs. defensive): 1-5

---

## Data Scientist: Methodology Rigor Test

**Setup**: Candidate describes an analysis, model, or data project.

**Challenge across dimensions:**

### Problem Framing
- "How did you define the problem? Who defined success?"
- "What was the business question behind the technical question?"
- "What would 'wrong' look like? How would you know?"

### Data Quality
- "Where did the data come from? What's missing?"
- "How did you handle missing values? Outliers?"
- "What biases exist in this dataset?"
- "How representative is your sample?"

### Methodology
- "Why this approach over alternatives?"
- "What assumptions does this method require?"
- "How did you validate those assumptions?"
- "Walk me through your feature engineering decisions."

### Evaluation
- "What metrics did you optimize for? Why those?"
- "How did you prevent overfitting?"
- "What's your confidence interval? Statistical significance?"
- "How did you validate in production vs. test?"

### Communication
- "How did you explain this to non-technical stakeholders?"
- "What pushback did you get? How did you address it?"
- "What did you simplify for the audience? What did you lose?"

**Scoring per response:**
- Statistical rigor: 1-5
- Awareness of limitations: 1-5
- Business translation (not just technical): 1-5

---

## UX Researcher: Evidence and Influence Test

**Setup**: Candidate describes a research project and its impact.

**Challenge across dimensions:**

### Study Design
- "Why this method over alternatives?"
- "What's your sample size? How did you recruit?"
- "What biases might affect your findings?"
- "How did you ensure you weren't leading participants?"

### Analysis
- "How did you synthesize across participants?"
- "What patterns emerged? What outliers did you see?"
- "How did you distinguish signal from noise?"
- "What surprised you vs. confirmed expectations?"

### Insight Quality
- "What's the 'so what' of this finding?"
- "How actionable is this insight?"
- "What does this NOT tell us?"
- "How confident are you? What would change your mind?"

### Influence
- "How did you get stakeholders to act on this?"
- "Who disagreed? How did you handle it?"
- "What research did NOT lead to change? Why?"
- "How did you prioritize which findings to push?"

### Ethics
- "How did you handle sensitive participant data?"
- "What consent did participants give?"
- "Were there findings you chose not to share? Why?"

**Scoring per response:**
- Methodological soundness: 1-5
- Insight actionability: 1-5
- Stakeholder influence: 1-5

---

## Operations / Business Ops: Systems Thinking Test

**Setup**: Candidate describes a process, system, or operational improvement they led.

**Challenge across dimensions:**

### Problem Diagnosis
- "How did you identify this was the right problem to solve?"
- "What was the root cause vs. symptoms?"
- "What data told you this was worth fixing?"
- "What was the cost of doing nothing?"

### Solution Design
- "What alternatives did you consider?"
- "Why this approach over others?"
- "What dependencies did you uncover?"
- "How did you handle edge cases?"

### Implementation
- "How did you roll this out? Phased or all at once?"
- "What resistance did you encounter?"
- "How did you get buy-in from stakeholders?"
- "What broke during implementation?"

### Measurement
- "How did you measure success?"
- "What leading vs. lagging indicators did you track?"
- "How did you isolate impact from other changes?"
- "What didn't improve that you expected to?"

### Sustainability
- "How did you ensure this stuck after you moved on?"
- "What documentation/training did you create?"
- "Who owns this now?"
- "What maintenance burden did you create?"

**Scoring per response:**
- Systems thinking (sees connections): 1-5
- Change management awareness: 1-5
- Measurement rigor: 1-5

---

## Marketing: Strategy and Attribution Test

**Setup**: Candidate describes a campaign, launch, or marketing initiative.

**Challenge across dimensions:**

### Strategy
- "Why this channel/approach over alternatives?"
- "Who was the target audience? How did you define them?"
- "What was the competitive context?"
- "How did this fit into the broader marketing strategy?"

### Execution
- "Walk me through the timeline and key decisions."
- "What did you have to cut or change mid-flight?"
- "How did you work with creative/product/sales?"
- "What was your budget? How did you allocate it?"

### Attribution
- "How did you measure impact?"
- "What was your attribution model? What are its flaws?"
- "How did you separate this campaign's impact from other factors?"
- "What metrics didn't move that you expected to?"

### Learning
- "What would you do differently?"
- "What did you learn about the audience?"
- "How did this inform future campaigns?"
- "What surprised you about performance?"

**Scoring per response:**
- Strategic clarity: 1-5
- Measurement sophistication: 1-5
- Learning orientation: 1-5

---

## General Drill Guidelines

1. **Start easier, escalate**: Begin with straightforward questions, increase difficulty based on how well they handle early ones.

2. **Push on "we"**: When candidate says "we," ask "What was YOUR specific contribution?"

3. **Probe the negative**: Every project has things that went wrong. If they only share positives, dig.

4. **Test adaptability**: Throw a curveball mid-answer: "Actually, let me ask about a different aspect..."

5. **Score in real-time**: Keep notes on where they're strong vs. struggling for debrief.
