# Worked Examples

These examples show what good skill output looks like. Use them as calibration anchors — not templates to copy, but demonstrations of the quality bar, triage logic, and coaching voice in action.

---

## Example 1: Scored Answer at Each Quality Level

**Question**: "Tell me about a time you had to make a difficult prioritization decision."

### Score 2 Answer (Weak)

> "We had a lot of projects going on and I had to figure out what to focus on. I looked at the data and decided to prioritize the most important ones. It worked out well and the team was happy with the results."

**Scores**: Substance 2 / Structure 2 / Relevance 2 / Credibility 1 / Differentiation 1
**Calibration band**: Mid-career

**Why these scores**:
- Substance 2: Vague claim ("looked at the data") with no specifics. What data? What projects? What was the trade-off?
- Structure 2: Has a beginning/middle/end but no narrative tension. Buries the actual decision.
- Relevance 2: Addresses the topic (prioritization) but doesn't answer what made it *difficult*.
- Credibility 1: No numbers, no specifics, no constraints. "It worked out well" — says who? How?
- Differentiation 1: Any candidate in any role could give this answer verbatim.

**Root cause pattern**: Conflict avoidance — the candidate stripped the story of all tension and difficulty, leaving a hollow shell.

---

### Score 3 Answer (Competent but Generic)

> "At my last company, I was managing three initiatives simultaneously — a new onboarding flow, a dashboard rebuild, and an API migration. We only had engineering bandwidth for two. I created a scoring framework based on user impact, engineering effort, and strategic alignment. The API migration scored lowest, so I deprioritized it. We shipped the onboarding flow and dashboard on time, and onboarding completion improved by 15%."

**Scores**: Substance 3 / Structure 3 / Relevance 3 / Credibility 3 / Differentiation 2
**Calibration band**: Mid-career

**Why these scores**:
- Substance 3: Specific claim with one metric (15%), but missing alternatives considered and the trade-off cost of deprioritizing the API migration.
- Structure 3: Clear STAR format but mechanical — setup → decision → result without narrative arc or tension.
- Relevance 3: Addresses the question but doesn't explain what made this *difficult*. Sounds like a straightforward framework application.
- Credibility 3: Has a number (15%) but no validation, no constraints mentioned, no acknowledgment of what was sacrificed.
- Differentiation 2: Uses a scoring framework — sounds like a PM textbook, not a specific person's judgment.

**Root cause pattern**: "Good enough" syndrome — specific enough to be credible, but stops before the interesting parts (the tension, the trade-off cost, the hard conversation with the API team).

---

### Score 4-5 Answer (Strong, Distinctive)

> "I was running three initiatives with bandwidth for two. The obvious cut was our API migration — lowest user-facing impact. But I'd learned from a previous mistake that deprioritizing infrastructure always compounds. Six months earlier, I'd made the 'user-first' call on a similar trade-off and we spent the next quarter firefighting tech debt.
>
> So instead of using a scoring framework, I went to the engineering lead and asked: 'If we delay the API migration by one quarter, what breaks?' Turns out, two partner integrations would stall, costing roughly $200K in committed revenue.
>
> I deprioritized the dashboard rebuild instead — controversial, because the VP of Sales had been promised it. I brought data to that conversation: the dashboard's primary users had built workarounds in Looker that covered 80% of the need. The remaining 20% could wait a quarter without measurable revenue impact.
>
> We shipped onboarding (+15% completion) and the API migration (preserving $200K in partner revenue). The dashboard shipped one quarter later with no measurable business impact from the delay. The earned lesson: prioritization frameworks are useful for the easy calls. For the hard ones, you need to understand the second-order cost of delay, not just the first-order impact of shipping."

**Scores**: Substance 5 / Structure 5 / Relevance 5 / Credibility 5 / Differentiation 4
**Calibration band**: Mid-career

**Why these scores**:
- Substance 5: Quantified impact ($200K, 15%, 80%), alternatives considered (dashboard vs. API), decision rationale with trade-offs, and outcome validated.
- Structure 5: Narrative arc with tension (the obvious call vs. the learned instinct), a twist (the counterintuitive choice), and a clean landing. Every sentence advances the story.
- Relevance 5: Directly addresses what made it *difficult* — it wasn't a framework problem, it was a judgment call where the "obvious" answer was wrong.
- Credibility 5: Numbers + constraints (VP promise) + realistic trade-offs (dashboard delay was manageable because of workarounds) + honest acknowledgment of a previous mistake.
- Differentiation 4: The earned secret ("prioritization frameworks work for easy calls, not hard ones") is genuine and specific. The previous-mistake callback shows self-awareness. One notch below 5 because the earned secret, while good, could be sharpened further.

---

## Example 2: Complete analyze Triage Decision in Action

**Context**: Mid-career PM, behavioral screen at a Series B startup. 6 questions in transcript.

**Aggregate scores**:
| Q# | Sub | Str | Rel | Cred | Diff |
|----|-----|-----|-----|------|------|
| 1  | 3   | 2   | 4   | 3    | 2    |
| 2  | 3   | 2   | 3   | 3    | 2    |
| 3  | 4   | 3   | 2   | 3    | 3    |
| 4  | 3   | 2   | 3   | 2    | 2    |
| 5  | 3   | 1   | 3   | 3    | 2    |
| 6  | 3   | 3   | 4   | 3    | 2    |
| **Avg** | **3.2** | **2.2** | **3.2** | **2.8** | **2.2** |

### Triage Decision

**Priority stack check**:
1. Relevance: Average 3.2. One answer at 2, rest at 3-4. Not the primary bottleneck.
2. Substance: Average 3.2. All at 3 except one 4. Not the primary bottleneck.
3. **Structure: Average 2.2. Four out of six answers below 3. This is the primary bottleneck.**
4. Credibility: Average 2.8. One answer at 2, rest at 3. Secondary concern.
5. Differentiation: Average 2.2. Five out of six below 3. Significant but lower priority than Structure.

**Multiple bottlenecks detected**: Structure AND Differentiation are both weak. Per priority stack, address Structure first — the candidate has content (Substance 3.2) but can't organize it under pressure. Differentiation work is premature until narrative architecture improves.

**Coaching path chosen**: Focus debrief on narrative architecture. Run constraint ladder drill immediately. Skip deep Calibration lens (the issue isn't word count or jargon — it's structure). Flag Differentiation as next priority after Structure reaches 3+.

**Psychological check**: No evidence of emotional bottleneck — scores are consistent between practice-like answers (no anxiety-driven variance visible in transcript).

**Debrief framing**: "Your content is solid — you have real experience and good examples. The issue is how you're packaging it. Four of your six answers lost their thread midway through. Let's focus entirely on narrative structure before we touch anything else. I want to run a constraint ladder drill on your weakest answer right now."

---

## Example 3: Practice Round Debrief with Self-Assessment Delta

**Drill**: Pushback drill (Stage 2)
**Question**: "Tell me about a time you disagreed with your manager."
**Interruption delivered**: At the 45-second mark — "I'm skeptical. It sounds like you just went along with their decision in the end."

### Round Debrief
- Drill: Pushback
- Objective: Maintain credibility and composure under skeptical challenge
- Candidate Self-Assessment: "I think I handled it okay — maybe a 3 on Credibility? I got a bit flustered by the interruption."

### What Worked
1. You acknowledged the skepticism directly instead of deflecting: "That's a fair read — let me clarify what actually happened." This is strong — I heard you pivot in real time and it landed.
2. Your opening structure was clean — you front-loaded the disagreement before the context.

### Gaps
1. After the interruption, you over-explained. Your response to the pushback was 90 seconds — the pushback response should be 20-30 seconds max. I could hear you shift from clarifying into defending.
2. The resolution was vague: "We eventually aligned." How? Who moved? What did you specifically do to influence the outcome?

### Scorecard
- Substance: 3 — Good setup, but the resolution lacks specifics.
- Structure: 3 — Strong opening, but the pushback response broke your structure.
- Relevance: 4 — Directly addressed disagreement and conflict.
- Credibility: 3 — The interruption response felt defensive rather than confident.
- Differentiation: 2 — This is a common "disagreed but aligned" story. What's the version only you could tell?

### Self-Assessment Delta
- Candidate rated themselves: ~3 on Credibility
- Coach scored: 3 on Credibility
- Calibration gap: Accurate on this one. Good self-awareness. (Note: across 4 prior rounds, candidate has consistently rated Structure 1 point higher than coach scores — this pattern hasn't surfaced here but is tracked in `coaching_state.md`.)

### Next Round Adjustment
- Try this single change: When interrupted with pushback, respond in ONE sentence that acknowledges and redirects, then continue your story. Practice: "Fair point — here's what makes this different: [one sentence]. So what I did was..." No more than 15 seconds on the pushback response.

---

## Example 4: Answer Rewrite Showing the Delta

**Original answer** (scored Substance 3 / Differentiation 2):
> "I led the migration of our payment system from Stripe to an in-house solution. It was a complex project involving multiple teams. We planned it carefully, executed in phases, and completed it on time. The new system saved us about 30% on transaction fees."

**Rewrite at 4-5 quality** (with annotations):

> "I led our payment migration from Stripe to in-house — **[ADDED: tension]** which our CTO initially vetoed because the risk-to-reward ratio looked terrible on paper. **[ADDED: specific constraint]** We were processing $4M/month through Stripe, and any migration downtime would cost us roughly $130K per day.
>
> **[ADDED: what made YOUR approach different]** What changed his mind was a phased migration plan I built where we ran both systems in parallel for 60 days — Stripe as primary, our system processing shadow transactions. That let us validate accuracy before any customer saw the switch. **[ADDED: earned secret]** The counterintuitive lesson: the parallel run cost us an extra $80K in dual fees, but it eliminated the single biggest risk. Most engineers I've talked to try to do payment migrations as a cutover. That's how you get outages.
>
> **[PRESERVED: outcome, with specificity added]** We completed the migration with zero downtime across 3 phases over 4 months. Transaction fees dropped 32% — saving roughly $1.3M annualized. **[ADDED: validation]** Our CFO cited it in the board deck as the highest-ROI infrastructure investment that year."

**What changed and why**:
- Added the CTO veto → creates tension, shows influence skills, makes it a *story* not a report
- Added the $4M/month and $130K/day → makes the stakes concrete and credible
- Added the parallel-run approach → shows *how* you solved it, not just that you did
- Added the earned secret about parallel vs. cutover → this is differentiation; only someone who's done this migration knows this
- Added the CFO board deck mention → third-party validation that costs nothing to add but dramatically increases credibility
- Preserved the 30% fee reduction but made it specific (32%, $1.3M annualized) → approximations with context > round numbers

**Note**: The candidate would need to supply the actual numbers (the $4M, $130K, 32%, $1.3M, and CFO detail). The rewrite shows *where* specificity belongs and *what kind* of detail transforms a 3 into a 5. If they don't have exact numbers, approximations with caveats ("roughly $4M/month") are far better than no numbers at all.

---

## Example 5: Kickoff Summary Showing Interview History Shaping Coaching Plan

**Context**: Mid-career PM, currently interviewing at 2 companies. Has done 3 interviews in the past month — advanced past one phone screen, rejected from two first rounds. Provided resume and target companies.

```markdown
## Kickoff Summary
- Track: Full System
- Target Role(s): Senior PM
- Seniority band: Mid-career (6 years experience)
- Timeline: Active — interviews this week and next
- Interview history: Active but not advancing (1/3 advance rate, both rejections at first round)
- Feedback Directness: 4
- Time-aware coaching mode: Focused

## Profile Snapshot (from resume analysis)
- Positioning strengths: Strong B2B SaaS trajectory, two promotions in 3 years at current company, owned a product line generating $8M ARR
- Likely interviewer concerns: No consumer product experience (both target companies have consumer-facing products); current title is "PM" not "Senior PM" — may need to demonstrate seniority through scope, not title
- Career narrative gaps: Transition from engineering (first 2 years) to product isn't explained — interviewers will ask, and you need a crisp 30-second version ready
- Story seeds: "Launched enterprise billing platform" bullet likely has a rich prioritization story; "Reduced churn by 18%" probably has a customer-insight story behind it

## Interview Readiness Assessment
Based on interview history and profile:
- Current readiness: Has foundation but gaps — advancing past some screens but not consistently
- Biggest risk going in: First-round rejections suggest either Relevance (answering the wrong question) or Structure (losing the thread). We need data to diagnose which.
- Biggest asset going in: Real quantified outcomes on resume — the raw material for strong stories exists

## First Plan
Your rejection pattern suggests a specific bottleneck — let's find it fast.

### Immediate (this session)
1. Build initial storybank with `stories` — capture 6-8 stories from your resume bullets, focusing on the enterprise billing and churn reduction experiences

### This week
2. Run `prep` for your nearest interview to sharpen question prediction and story mapping
3. Run `practice ladder` — if Structure is the bottleneck (my hypothesis from first-round rejections), this will surface it in one session

### Before next interview
4. If you have a transcript from any rejected interview, run `analyze` — that's the fastest way to diagnose exactly what's not landing

**Next commands**: `stories`, `prep [company]`, `practice ladder`, `help`
```

**Why this example matters**: Notice how interview history (active but not advancing, first-round rejections) shapes the entire plan. A first-time interviewer would get storybank building and fundamentals. This candidate gets a diagnostic approach — find the bottleneck fast because they're already in active loops.

---

## Example 6: Prep Brief Excerpt Showing Format-Specific Coaching

**Context**: Senior Engineer preparing for a system design interview at a growth-stage startup. Format discovered through recruiter: 50-minute live verbal walkthrough, collaborative (interviewer participates), one problem.

```markdown
## Prep Brief: TechCo — Senior Software Engineer

## Interview Format
- Identified format: System design / case study (verbal walkthrough, collaborative)
- Format source: Candidate confirmed with recruiter — 50 min, one problem, collaborative with senior engineer
- Format-specific guidance: Collaborative formats reward thinking out loud and engaging with the interviewer's suggestions. Treat them as a partner, not an audience. Ask clarifying questions early, propose your approach, and invite feedback: "Does this direction make sense, or should I consider a different angle?"
- Scoring weight adjustments for this format: Structure + Substance weighted highest. Credibility scored on process rigor, not answer correctness.
- Coaching scope: I'll help you practice structuring your thinking visibly, scoping problems before diving in, narrating tradeoffs, and engaging with probing questions. I won't evaluate the technical correctness of your designs — for that, practice with a senior engineer peer.

## Company Culture Read
- Known culture signals: Careers page emphasizes "pragmatic engineering" and "shipping over perfection" (Tier 1 — from their careers page)
- What this company rewards in interviews: Based on the JD's emphasis on "end-to-end ownership" and "working in ambiguity," they likely value candidates who can make decisions with incomplete information and explain their reasoning (Tier 1 — from JD)
- What to avoid: Over-engineering in the system design. Their careers page mentions "right-sized solutions" twice — don't propose Google-scale architecture for a growth-stage problem.
- What I don't know: Specific evaluation rubric, whether they use a scoring sheet, how much weight they put on system design vs. other rounds
- Confidence in culture read: Medium
- Sources used: Company careers page, JD, candidate input from recruiter conversation
```

**Why this example matters**: Shows how coaching scope boundaries appear naturally inside a prep brief — not as a disclaimer, but as a clear statement of what this coaching session can and can't deliver. Also demonstrates tiered sourcing (Tier 1 labels on every claim).

---

## Example 7: Mock Debrief Excerpt Showing Holistic Arc Analysis

**Context**: Mid-career PM, mock behavioral screen (5 questions), targeting Series B startup.

```markdown
## Mock Interview Debrief: Behavioral Screen — GrowthCo PM

## Overall Impression
- Hiring signal: Mixed
- One-sentence summary: Strong individual answers but the full interview felt like five separate stories rather than a cohesive narrative about who you are as a PM.

## Arc Analysis
- Energy trajectory: Started high → Ended medium. Your first two answers had genuine enthusiasm; by Q4 you sounded like you were reciting. The energy dip was visible.
- Story diversity: 4 unique stories across 5 questions (S003 used twice — for prioritization and for conflict)
- Pacing: Front-loaded. First two answers were 90 seconds each (ideal). Last two crept to 3+ minutes. The interviewer would notice.
- Answer length distribution: Erratic — 85 words, 110 words, 280 words, 320 words, 190 words. The middle answers ballooned.

## Holistic Patterns (things only visible across the full interview)
- Repeated crutch phrases: "At the end of the day" appeared in 4 of 5 answers. "So basically" appeared in 3. These become invisible to you but very visible to the interviewer.
- Topics avoided: No answer mentioned failure, disagreement with a manager, or something that went wrong. Five stories, all successes. An interviewer would wonder: "Does this person only tell me what went right?"
- Questions that caused visible hesitation: Q4 ("Tell me about a time you failed") — you hesitated, then told a success story with a minor setback. This is a red flag for self-awareness.
- Best moment of the interview: Q2, where you described killing a feature despite sales pressure. The earned secret about small-scale success being a political trap was memorable.
- Worst moment and recovery quality: Q4. You didn't recover — the answer wandered and never landed a clear takeaway. Recovery: poor.

## Signal Reading Notes
- Q1: Follow-up indicated interest ("Tell me more about the data you used") — positive signal
- Q3: Interviewer moved on quickly after your answer — likely negative, your impact statement was vague
- Q4: Interviewer rephrased the question ("I mean a real failure, not just a challenge") — your first answer didn't land

## Interviewer Perspective
- "When you used S003 for both the prioritization and conflict questions, I noticed. It made me wonder if you have a limited range of experiences to draw from. In a real interview, I'd probe: 'Can you give me a different example?'"
- "On Q4, I rephrased because your answer was about overcoming a challenge, not about failing. When a candidate can't name a real failure, I worry about self-awareness and honesty."
- "Your Q2 answer was the one I'd bring to the hiring committee. The earned secret about features becoming politically unkillable at small scale — that stuck with me."

## Top 3 Changes for Next Mock
1. Prepare a genuine failure story. Not a "challenge I overcame" — a real mistake with real consequences and a real lesson. Run `stories add` and specifically mine for failure experiences.
2. Practice ending answers. Your last sentence should be a clear takeaway, not a trail-off. Use the constraint ladder to practice the last 15 seconds of each story.
3. Vary your stories. You have 8 stories in your storybank — use them. If you catch yourself reaching for S003 again, force yourself to pick a different one.

**Next commands**: `stories add`, `practice ladder`, `mock behavioral`
```

**Why this example matters**: Demonstrates the holistic patterns only visible across a full mock — story recycling, energy trajectory, crutch phrases, topic avoidance. Individual answer scoring misses these. Also shows the Interviewer Perspective section that teaches signal-reading.

---

## Example 8: Progress Review Showing Trend Narration

**Context**: Senior Designer, 5 sessions completed, 2 real interviews (1 advanced, 1 rejected). Running `progress` after 5th session.

```markdown
## Progress Snapshot
- Sessions analyzed: 5
- Real interviews completed: 2
- Real interview outcomes: 1 advanced / 1 rejected / 0 pending
- Current trend: Improving (with one plateau)

## Your Trajectory

- Substance: 2.5 → 3.0 → 3.5 → 3.5 → 4.0 — Steady climb. The jump from 2.5 to 3.0 happened in session 2 when you started adding user research numbers to your stories instead of just saying "we did research." The plateau at 3.5 broke in session 5 when you added alternatives-considered framing ("We tested three approaches..."). Keep doing that — it's working.

- Structure: 3.0 → 3.0 → 3.5 → 4.0 → 4.0 — This was your first plateau and your first breakthrough. Structure didn't move until you practiced the constraint ladder in session 3. Once you learned to front-load the headline, everything tightened. You're now consistent at 4 — this is no longer a priority.

- Relevance: 3.5 → 3.5 → 4.0 → 4.0 → 4.0 — Started strong, now solid. Your question-decoding improved naturally as Structure improved — once you knew where the answer was going, you stopped adding irrelevant context. No further action needed here.

- Credibility: 2.0 → 2.5 → 3.0 → 3.0 → 3.5 — The slowest climber. Your root cause: modesty. You consistently undersell your individual contribution. The I/we audit in session 3 helped (2.5 → 3.0), but you've slipped back into "we" framing in the last two sessions. This needs active monitoring.

- Differentiation: 2.0 → 2.0 → 2.5 → 3.0 → 3.5 — This didn't move at all until session 4, when we extracted your first earned secret ("The best design systems are built from constraints, not inspiration"). That was the unlock. You now have 3 earned secrets across your storybank. The next level (4+) requires weaving them into your answers naturally — right now they feel bolted on.

## Self-Assessment Calibration
- Your average self-ratings vs. my scores:
  - Substance: You 3.8 / Me 3.5
  - Structure: You 3.5 / Me 3.7
  - Relevance: You 3.8 / Me 3.9
  - Credibility: You 3.5 / Me 2.8
  - Differentiation: You 2.5 / Me 2.6
- Pattern: Over-rater on Credibility (by 0.7 — significant gap)
- What this means: You think your individual contribution is coming through more clearly than it actually is. This is consistent with the modesty pattern — you believe you're claiming credit, but the interviewer isn't hearing it. The gap between your perception and reality on this dimension is the single most important thing to close.

## Graduation Check
- Interview-ready criteria: 4 of 6 met
  - [x] 3+ scores of 4+ across dimensions (Substance, Structure, Relevance all hit 4)
  - [x] No dimension consistently below 3 (all at 3+)
  - [ ] 8+ stories, 5+ rated 4+ strength (you have 7 stories, only 3 at 4+)
  - [x] Critical competency gaps covered
  - [ ] Gap questions handled in practice (not yet tested)
  - [x] Self-assessment calibrated within 0.5 (except Credibility — flagged above)
- Assessment: Almost ready — two gaps to close before your next round

## Top 2 Priorities (Next 2 Weeks)
1. Priority: Close the Credibility gap — I/we audit on your top 5 stories
   Why: Your self-assessment is 0.7 points off on this dimension, and the real interview rejection feedback ("hard to tell what she specifically did") confirms it
   Drill: I/we audit + constraint practice on your 3 weakest-Credibility stories
   Success metric: Credibility ≥ 3.5 in next practice with self-assessment within 0.3 of coach score

2. Priority: Add 2 more stories to storybank and extract earned secrets
   Why: 7 stories with only 3 at 4+ isn't enough for a competitive process. You need at least 5 at 4+ to have options.
   Drill: `stories add` focusing on failure/learning experiences (your storybank has zero)
   Success metric: 9+ stories, 5+ at 4+ strength, at least 1 failure story
```

**Why this example matters**: Shows trend narration as a story, not a spreadsheet. Each dimension gets a narrative arc with inflection points, causes, and next unlocks. The self-assessment calibration section reveals the most actionable insight (Credibility over-rating by 0.7 = the candidate thinks they're claiming credit but they're not). The graduation check makes readiness concrete.

---

## Example 9: Debrief Showing Emotional Triage and Capture

**Context**: Mid-career Engineer, just finished a system design round at a growth-stage company. Came to the session 90 minutes after the interview. Previous debrief exists for Round 1 (behavioral screen, advanced).

```markdown
## Interview Debrief: TechCo — Round 2 (System Design)
- Date: 2024-03-15
- Interviewer(s): Senior Staff Engineer (name unknown)
- Format: System design — verbal walkthrough, collaborative, 50 min
- Emotional read: "Rough" — candidate described feeling lost midway through

## Emotional Triage
Your one-word was "rough" — let's capture what happened while it's fresh. We can analyze later when there's some distance. Right now, the goal is data capture, not judgment.

## Questions Recalled
1. "Design a rate-limiting system for our API."
   - Self-assessment: Rough
   - Story used: None (technical design, not behavioral)
   - Notes: Jumped straight into solution without asking clarifying questions. Realized halfway through that assumptions about scale were wrong. Interviewer redirected: "What if the traffic is 100x what you assumed?"

2. Follow-up: "Walk me through how you'd handle distributed rate limiting across multiple regions."
   - Self-assessment: Okay
   - Story used: Referenced S004 (multi-region migration) for context on distributed systems experience
   - Notes: Felt more grounded here because it connected to real experience. Interviewer engaged — asked 2 follow-ups.

3. "What are the tradeoffs between token bucket and sliding window approaches?"
   - Self-assessment: Strong
   - Story used: None
   - Notes: This is an area of genuine expertise. Explained clearly, named tradeoffs unprompted.

## Interviewer Signals Observed
- Positive signals: Engaged on Q2 follow-ups, nodded during Q3 tradeoff explanation
- Negative signals: On Q1, interviewer let candidate go for 3 minutes before redirecting — likely waiting to see if candidate would self-correct (they didn't)
- Neutral/ambiguous: Interviewer was quiet for first 5 minutes — could be their style or a reaction to the weak opening

## Surprises
- The problem was simpler than expected — the difficulty came from the follow-ups, not the initial design
- Interviewer cared more about the reasoning process than the architecture diagram
- Silence after Q1 was unnerving — candidate filled it with rambling instead of pausing to regroup

## Stories Used
| Story | Question | How It Landed (candidate read) |
|-------|----------|-------------------------------|
| S004 (multi-region migration) | Q2 follow-up | Landed well — grounded the technical discussion in real experience |

## Candidate's Own Takeaways
- What to do differently: "I need to ask clarifying questions before diving in. I knew this and still didn't do it."
- What worked: "When I connected the design to my actual experience (S004), it felt real and the interviewer responded to it."

## Coaching Notes
The pattern here is clear and consistent with what we saw in practice: you skip the clarification phase when you feel time pressure. In practice, your Clarification-Seeking drill scores were 2-3 — and that's exactly what showed up in the real interview. The good news: Q2 and Q3 showed that once you're grounded in real experience, your communication quality jumps. The intervention is specific: the first 3 minutes of any system design interview should be questions, not solutions.

## Coaching State Updates
- Storybank updates: S004 Last Used = 2024-03-15, performance note: "landed well in system design context — grounds technical discussion in real experience"
- Interview Loop updates: Round 2 completed, format confirmed (verbal walkthrough, collaborative), clarification-skipping pattern confirmed in real interview

## Transcript Status
- [ ] Transcript available → run `analyze` when ready
- [x] No transcript → directional analysis above is what we have

**Next commands**: `practice technical` (Clarification-Seeking drill specifically), `hype` (if Round 3 is scheduled), `progress`
```

**Why this example matters**: Shows the emotional triage decision in action — candidate said "rough," so the coach prioritized capture over coaching. The pattern connection between practice scores (Clarification-Seeking at 2-3) and real interview behavior (skipped clarification on Q1) demonstrates why practice data matters. The coaching note is direct but constructive. Also shows how a technical interview debrief differs from a behavioral one — story usage is sparse, and the observations focus on process (clarification, reasoning narration) rather than content.

---

## Example 10: Analyze Excerpt Showing Signal-Reading in Transcript

**Context**: Mid-career PM, behavioral screen. Showing signal-reading analysis integrated into per-answer scoring for two questions.

```markdown
### Q2: "Tell me about a time you had to influence a decision without direct authority."
- Scores: Substance 4 / Structure 4 / Relevance 4 / Credibility 3 / Differentiation 3
- Hire Signal contribution: Positive
- What worked: Strong specific example — you named the VP you influenced, the data you brought, and the meeting where the decision turned. Front-loaded the outcome ("I changed the roadmap priority for Q3") before the backstory.
- Biggest gap: Credibility dips because you said "the team eventually came around" without explaining HOW you influenced them. The mechanism of influence is exactly what this question tests.
- Root cause pattern: Reflexive "we" framing — "we aligned on the new direction" obscures that YOU drove the alignment.
- Tight rewrite direction: Replace "we aligned" with the specific action: "I set up a 1:1 with the VP, walked through the customer data showing churn correlated with the feature gap, and proposed a 2-week spike. She approved it in that meeting."

**Signal-reading observations for Q2:**
- The interviewer asked two follow-ups: "How did you get the VP's time?" and "What data did you use?" — both are positive signals. When interviewers drill into the HOW of influence, they're interested, not skeptical. They want more detail because the story is landing.
- After your answer about data, the interviewer said "That's helpful, thank you" — this is a wrap-up signal, not enthusiasm. They got what they needed and were ready to move on. You correctly didn't over-extend.

### Q4: "Describe a situation where you disagreed with your manager's decision."
- Scores: Substance 2 / Structure 3 / Relevance 2 / Credibility 2 / Differentiation 1
- Hire Signal contribution: Negative
- What worked: You structured the answer clearly — setup, disagreement, resolution. The bones were there.
- Biggest gap: Relevance. You described a situation where you "pushed back" but ultimately agreed your manager was right. That's a challenge story, not a disagreement story. The question specifically asks about disagreement — they want to know what happens when you think the boss is wrong AND you still think so after the conversation.
- Root cause pattern: Conflict avoidance — you selected a "safe" disagreement where you were the one who changed their mind, which avoids showing real tension.
- Tight rewrite direction: Use a story where the disagreement persisted. Even better: a case where you escalated, compromised, or proceeded despite disagreement with clear rationale.

**Signal-reading observations for Q4:**
- The interviewer paused for 3 seconds after your answer, then said "Okay" and moved directly to Q5. No follow-up, no "tell me more," no probing. This is a negative signal — the answer didn't generate interest. Compare this to Q2 where you got two engaged follow-ups. The contrast is itself data: Q2's specificity pulled the interviewer in; Q4's safety pushed them away.
- The quick pivot to Q5 also suggests the interviewer may have mentally scored this answer and decided probing wouldn't yield better material. In real-time, this is your cue that the answer didn't land — if you notice it, you can volunteer: "Actually, I have a stronger example of real disagreement — would you like to hear it?" Most interviewers will say yes.
```

**Why this example matters**: Shows how signal-reading observations are woven into per-answer analysis, not bolted on as a separate section. The Q2 analysis distinguishes between positive follow-ups (interest) and wrap-up signals (they got enough). The Q4 analysis uses the contrast between the two answers to teach the candidate to read real-time signals — the silence + quick pivot = the answer didn't land. Also demonstrates the coaching move of suggesting a real-time recovery: offering a stronger example mid-interview.
