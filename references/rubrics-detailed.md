# Detailed Scoring Rubrics

Use these expanded rubrics when doing deep analysis. The compact version in SKILL.md is for quick scoring.

## Substance (Evidence Quality)

| Score | Description | Example |
|-------|-------------|---------|
| 1 | Generic platitude, no evidence | "I'm a great collaborator" |
| 2 | Vague claim with weak support | "I improved the process and people liked it" |
| 3 | Specific claim, missing quantification | "I redesigned the onboarding flow and reduced drop-off" |
| 4 | Quantified with context, missing alternatives considered | "I redesigned onboarding, reducing drop-off from 40% to 25%" |
| 5 | Quantified + alternatives weighed + decision rationale + outcome | "I redesigned onboarding after testing 3 approaches. We chose progressive disclosure over a wizard because our data showed users abandoned at step 3. Drop-off fell from 40% to 25%, validated over 10K users." |

**Coaching notes:**
- Push for numbers even when "hard to measure" — approximations with caveats are better than none
- Ask: "What would a skeptic say is missing from this evidence?"
- Flag when impact is claimed without explaining the candidate's specific contribution

## Structure (Narrative Clarity)

| Score | Description | Example |
|-------|-------------|---------|
| 1 | Stream of consciousness, no clear point | Jumps between topics, listener lost |
| 2 | Central idea unclear until the end | Buries the lead, meanders to point |
| 3 | Clear structure but missing transitions | Good STAR but choppy between sections |
| 4 | Well-structured with smooth flow, minor tangents | Clean arc with one or two detours |
| 5 | Crisp structure: setup → conflict → resolution → impact | Every sentence advances the story, lands with clear takeaway |

**Coaching notes:**
- Best answers front-load the headline: "The key learning was X. Here's how we got there..."
- Tangents often signal the candidate is unsure what matters — help them identify the core
- Practice the "30-second version" to force clarity on what's essential

## Relevance (Question Fit)

| Score | Description | Example |
|-------|-------------|---------|
| 1 | Doesn't address the question asked | Asked about conflict, answers about a successful launch |
| 2 | Tangentially related, misses core of question | Asked about failure, talks about a challenge they overcame easily |
| 3 | Addresses question but includes irrelevant details | Right story but 40% of answer is background noise |
| 4 | Directly addresses question with minor drift | Solid answer with one unnecessary tangent |
| 5 | Laser-focused, every sentence serves the answer | Could not remove a single sentence without losing value |

**Coaching notes:**
- Restate the question before answering to ensure alignment
- Common failure: using a "favorite" story that doesn't quite fit
- Ask: "If the interviewer only remembers one thing, what should it be?"

## Credibility (Believability)

| Score | Description | Example |
|-------|-------------|---------|
| 1 | Claims with no support or obvious exaggeration | "I single-handedly transformed the company culture" |
| 2 | Support is vague or generic | "I increased efficiency" (how? by how much?) |
| 3 | Specific details but missing numbers or outcomes | "I built a dashboard the team used daily" (impact?) |
| 4 | Quantified with context, could use stronger proof points | "Dashboard reduced reporting time by 50%" (validated how?) |
| 5 | Numbers + artifacts + validation from others + realistic constraints | "Dashboard cut reporting from 4 hours to 2 (verified by team survey). CEO mentioned it in all-hands. Took 3 weeks to build with one engineer." |

**Coaching notes:**
- Credibility increases when candidate acknowledges constraints and trade-offs
- Third-party validation (quotes, awards, metrics from others) strengthens claims
- Watch for "we" vs "I" confusion — interviewers want to know the candidate's specific role
- Realistic timelines and resource constraints make stories more believable

---

## Aggregate Scoring

After scoring individual answers:

### Interview-Level Assessment

| Rating | Criteria |
|--------|----------|
| **Strong Hire** | Multiple 4-5 scores, no major gaps, demonstrated unique value |
| **Hire** | Mostly 3-4 scores, minor gaps that could be coached |
| **Mixed** | Inconsistent scores, some strengths but concerning gaps |
| **No Hire** | Multiple low scores, significant evidence gaps, or red flags |

### Trend Analysis (across multiple interviews)

Track average scores per dimension over time:
- Improving: +0.5 or more from baseline
- Stagnant: Within ±0.3 of baseline
- Declining: -0.5 or more from baseline

Stagnant scores after 3+ interviews signal need to change approach, not just practice more.
